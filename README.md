# ğŸ§  PEDESTRIAN_NAV_AI
### Pedestrian-Aware Autonomous Navigation using ROS2, Gazebo & Deep Learning

> From sensing â†’ understanding â†’ tracking â†’ risk estimation â†’ safe decision making.

---

## ğŸ“˜ Project Overview

**PEDESTRIAN_NAV_AI** is a complete **end-to-end pedestrian safety framework** for autonomous vehicles built using:

- ROS 2 Humble
- Gazebo Simulation
- LiDAR + Camera fusion
- YOLOv8 perception
- BEV (Birdâ€™s-Eye-View) world modeling
- Tracking + velocity smoothing
- Time-To-Collision (TTC) risk logic
- Stop-and-Go autonomous safety control

This system goes **beyond object detection** and implements a **world-aware, safety-critical decision layer** capable of:

âœ” Detecting pedestrians  
âœ” Tracking motion in world frame  
âœ” Estimating collision risk  
âœ” Making real-time braking decisions  
âœ” Explaining decisions visually  

---

## ğŸ“ Academic Context

**M.Tech Final Dissertation â€“ BITS Pilani (WILP)**  
**Title:** Pedestrian-Aware Autonomous Navigation: Integrating Deep Vision, Multi-Modal Fusion, and Uncertainty-Aware Planning  

This repository contains the **actual implementation used in the thesis experiments**, not a toy demo.

---

## ğŸš€ Final System Capabilities (Current Status)

### âœ… Fully Implemented

### Perception
- YOLOv8 pedestrian detection
- Custom optimized model (3.5ms inference)
- Camera â†’ bounding boxes â†’ tracked targets

### Sensor Fusion
- Geometric projection (baseline)
- **BEV Fusion (primary production pipeline)**
- World-frame Cartesian coordinates

### Tracking
- Nearest Neighbor association
- Stable ID persistence
- EMA velocity smoothing (Î± = 0.3)

### Risk Estimation
- Time-To-Collision (TTC)
- Lane filtering
- Collision radius modeling
- Multi-threshold safety margins

### Safety Decision Layer (COMPLETE)
- `/safety_decision_node`
- Outputs:
  - GO
  - SLOW
  - STOP
- Real-time braking logic validated at **35 m/s**

### Visualization
- RViz
- BEV radar view
- Health dashboard
- Live decision state monitor

### Evaluation
- ADE / FDE metrics
- Fusion comparison (Geometry vs BEV)
- EMA vs LSTM benchmarking
- CSV logging

### Reproducibility
- Docker support
- Full ROS2 workspace
- Modular packages
- Plug-and-play design

---

## ğŸ—ï¸ Final Architecture

```
Gazebo Simulation
â†“
Sensors (Camera + LiDAR)
â†“
YOLOv8 Perception
â†“
BEV Fusion (World Frame)
â†“
Tracking + EMA smoothing
â†“
TTC Risk Estimation
â†“
Safety Decision Node
â†“
GO / SLOW / STOP
```

---

## ğŸ“‚ Workspace Structure

```
PEDESTRAN_NAV_AI/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ av_gazebo_bringup/ # Simulation, worlds, launch files
â”‚ â”œâ”€â”€ av_sensor_package/ # YOLO + fusion + tracking
â”‚ â””â”€â”€ risk_assesment/ # TTC + Stop-Go + evaluation
â”‚
â”œâ”€â”€ models/
â”œâ”€â”€ docs/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ results
```

---

## ğŸ“¦ Package Responsibilities

### av_gazebo_bringup
- World files
- URDF ego vehicle
- Pedestrian spawners
- Scenario evaluator
- System launchers

### av_sensor_package
- YOLO node
- BEV fusion
- World projection
- Tracking
- Visualizers

### risk_assesment
- TTC computation
- EMA smoothing
- Safety decision logic
- Metrics + benchmarking

## ğŸ“š Package & Module READMEs

For quick navigation to package-level and module-level documentation, see:

- [src/av_gazebo_bringup/README.md](src/av_gazebo_bringup/README.md)
- [src/av_sensor_package/README.md](src/av_sensor_package/README.md)
- [src/av_sensor_package/av_sensor_package/README.md](src/av_sensor_package/av_sensor_package/README.md)
- [src/risk_assesment/README.md](src/risk_assesment/README.md)
- [src/risk_assesment/risk_assesment/README.md](src/risk_assesment/risk_assesment/README.md)
- [doc/av_gazebo_bringup/README.md](doc/av_gazebo_bringup/README.md)
- [doc/ros_yolo_setup.md](doc/ros_yolo_setup.md)
---

## âš™ï¸ Build & Run

### Native

```bash
colcon build
source install/setup.bash
ros2 launch av_gazebo_bringup system_integration.launch.py
```
### Only world
```
ros2 launch av_gazebo_bringup av_world.launch.py
```
### Individual nodes
```
python -m av_sensor_package.yolo_node
python -m risk_assesment.safety_decision_node
```
### ğŸ³ Docker (Recommended)
```
Build:

docker build -t pedestrian_nav_ai .


Run:

docker-compose up --build

```
### Supports:

* GUI Gazebo

* GPU acceleration (optional)

* Reproducible environment



### ğŸ“Š Key Experimental Results
**Fusion Comparison**
| Metric              | Geometry     | BEV        |
| ------------------- | ------------ | ---------- |
| Stability           | Flicker      | Stable     |
| Occlusion handling  | Poor         | Excellent  |
| High-speed (35 m/s) | Delayed stop | Early stop |

**Tracking**
| Model | ADE   |
| ----- | ----- |
| EMA   | 0.7 m |
| LSTM  | 17+ m |

**â¡ EMA chosen for safety-critical reliability**
| Model   | mAP      | Speed      |
| ------- | -------- | ---------- |
| YOLOv8n | 0.58     | 4.4 ms     |
| Custom  | **0.69** | **3.5 ms** |

### ğŸ§  Design Decisions
Why BEV Fusion?

      Better spatial reasoning + occlusion robustness

Why EMA over LSTM?

* Deterministic

* Stable

* No training instability

* Lower latency

* Safer for real-time braking

Why Modular ROS2?

* Simulator independent

* Easy hardware porting

* Debuggable safety nodes

ğŸ”® Future Improvements (Optional Research)

* Transformer-based intent prediction

* Radar fusion

* Probabilistic planning

* Lateral avoidance (swerve)

* Factor-graph state estimation (GTSAM)

### ğŸ‘¨â€ğŸ’» Author
Nripender Kumar
M.Tech AI/ML â€“ BITS Pilani
Autonomous Systems & Robotics

ğŸ“œ License

MIT License â€“ Academic & Research Use

